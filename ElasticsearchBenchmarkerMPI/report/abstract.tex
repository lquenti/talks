Due to current developments in simulation, data science, and machine learning, research has become ever more data-driven and compute-intensive. Especially, data lakes are evermore important, due to the decreased prices in storage with less specialized and more heterogeneous hardware support. To manage the sheer amount of raw, unprocessed data, a both sufficient and performant metadata management solution is paramount.

After an internal performance evaluation of Elasticsearch for data lake metadata management at the GWDG, it was found out that rally \cite{rally}, Elastic's official benchmarker, does not scale well enough to simulate a realistic \ac{HPC} workload. This report developed an \acs{MPI}-based, \acs{HPC}-native benchmarking framework for evaluating Elasticsearchs performance. A distributed, I/O-optimized ingestion benchmarker was designed and implemented, which measures both the latency and write throughput when ingesting large corpora of documents. Furthermore, a distributed query benchmarker was designed, which allows for custom scenarios using a newly developed, \acs{JSON}-based \ac{DSL} embedding the Elastcisearch Search \acs{API} query language. All syntax is conceptually similar to rally, which makes porting those battle-tested benchmarks trivial.

Additionally, an \acs{MPI}-based, zero-configuration, stateful workflow to automatically create and (re-)spawn an Elasticsearch cluster in a dynamic \acs{SLURM} environment. By dynamically patching the Elasticsearch configuration used in the Singularity containers with the new hostnames assigned by SLURM, the cluster becomes hardware-independent and can easily be reused in future jobs. Lastly, a full end-to-end benchmarking workflow was designed.

The benchmarker was tested on the Emmy \acs{HPC} cluster using 3 nodes. For this, Elastic's \texttt{nyc\_taxis} benchmark track was successfully ported and extended with further scenarios. It can be seen that the benchmark successfully records the expected scaling behaviour when increasing the number of load generators per cluster node.

In conclusion, the benchmarker successfully provides a large-scale alternative to the canonical rally benchmarker, allowing further research in \ac{HPC}-native, high-throughput use case benchmarking for data lake applications.
